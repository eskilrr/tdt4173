{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the packages\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data and splitting it into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the clean data into test and train and separating out the target values.\n",
    "# Also dropping some variable we dont need\n",
    "\n",
    "train = pd.read_csv(r'C:\\Users\\eskil\\PycharmProjects\\tdt4173\\Data\\clean\\train_data_clean', sep = \",\")\n",
    "test = pd.read_csv(r'C:\\Users\\eskil\\PycharmProjects\\tdt4173\\Data\\clean\\test_data_clean', sep = \",\")\n",
    "\n",
    "train = train.drop([\"artist_followers\"], axis = 1)\n",
    "test = test.drop([\"artist_followers\"], axis = 1)\n",
    "\n",
    "\n",
    "x_train = train.drop([\"target\", \"Unnamed: 0\"], axis = 1)\n",
    "y_train = train[\"target\"]\n",
    "\n",
    "x_test = test.drop([\"target\", \"Unnamed: 0\"], axis = 1)\n",
    "y_test = test[\"target\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the different parameters\n",
    "Tuning the hyperparameters and scaling the data used by the cross validation (CV) to avoid any dataleakage when scaling the data. Not using the OOB-score considering because it uses only a subset of the DTs in the random forest not used when training and we also have enough data to set aside a considerably large validationset when using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning the n_estimators-parameter to find where it starts to drop off.\n",
    "n_estimators = [1, 2, 5, 10, 50, 100, 150, 200, 300, 400, 500]\n",
    "max_depth = [20]\n",
    "param_grid = {\"n_estimators\" : n_estimators, \"max_depth\" : max_depth}\n",
    "estimator = RandomForestClassifier()\n",
    "clf = GridSearchCV(estimator = estimator, param_grid = param_grid, n_jobs=-1, cv = 5)\n",
    "clf.fit(x_train,y_train)\n",
    "allscores=clf.cv_results_['mean_test_score']\n",
    "print(allscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning the max_depth parameter.\n",
    "n_estimators = [150]\n",
    "max_depth = [1,2,5,10,12,15,17,20,25,30,40, None]\n",
    "param_grid = {\"n_estimators\" : n_estimators, \"max_depth\" : max_depth}\n",
    "estimator = RandomForestClassifier()\n",
    "clf = GridSearchCV(estimator = estimator, param_grid = param_grid, n_jobs=-1, cv = 5)\n",
    "clf.fit(x_train,y_train)\n",
    "allscores=clf.cv_results_['mean_test_score']\n",
    "print(allscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proving how our model starts to overfit the trainingdata severely for large depth values,\n",
    "#without any increase in generalization/accuracy on the \"test data\"\n",
    "#for this we don't use cross validation, considering the increasing time complexity and that its mainly for visualisation purposes\n",
    "train_depth, evaluation_depth = train_test_split(train, test_size = 0.2, random_state = 4)\n",
    "\n",
    "x_train_depth = train_depth.drop([\"target\", \"Unnamed: 0\"], axis = 1)\n",
    "y_train_depth = train_depth[\"target\"]\n",
    "\n",
    "x_eval_depth = evaluation_depth.drop([\"target\", \"Unnamed: 0\"], axis = 1)\n",
    "y_eval_depth = evaluation_depth[\"target\"]\n",
    "\n",
    "max_depths = np.linspace(1, 25, 25, endpoint=True)\n",
    "train_results = []\n",
    "eval_results = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    rf = RandomForestClassifier(max_depth=max_depth, n_jobs=-1, n_estimators = 150)\n",
    "    rf.fit(x_train_depth, y_train_depth)\n",
    "    \n",
    "    acc_rf = round(rf.score(x_train_depth, y_train_depth) * 100, 2)\n",
    "    train_results.append(acc_rf)\n",
    "\n",
    "    acc_rf = round(rf.score(x_eval_depth, y_eval_depth) * 100, 2)\n",
    "    eval_results.append(acc_rf)\n",
    "    \n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "print(train_results)\n",
    "print(eval_results)\n",
    "plt.plot(max_depths, train_results)\n",
    "plt.plot(max_depths, eval_results)\n",
    "plt.grid()\n",
    "\n",
    "plt.title(\"Accuracy train/test for different depth\")\n",
    "plt.xlabel(\"depth\")\n",
    "plt.ylabel(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting all the parameters to be tested as well as initialize the classifier to be used.\n",
    "clf = RandomForestClassifier()\n",
    "min_samples_split = [2, 4, 6, 8, 12, 16, 20]\n",
    "min_samples_leaf = [1, 3, 5, 9, 15, 27]\n",
    "max_features = [\"sqrt\", 0.5, 0.7]\n",
    "n_estimators = [150]\n",
    "max_depth = [15]\n",
    "criterion = [\"gini\"]\n",
    "\n",
    "#Making a grid of all parameters and use the gridsearchCV to crossvalidate ahe models using all combinations inside the grid\n",
    "# Printing the params yielding the best accuracy.\n",
    "param_grid = {\"criterion\" : criterion, \"n_estimators\" : n_estimators,\n",
    "              \"max_depth\" : max_depth, \"min_samples_split\" : min_samples_split,\n",
    "              \"min_samples_leaf\" : min_samples_leaf, \"max_features\" : max_features}\n",
    "clf = GridSearchCV(estimator = clf, param_grid = param_grid, n_jobs=-1, cv = 5)\n",
    "clf.fit(x_train, y_train)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model with the best parameters, printing the accuracies and plotting the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the model on the hold off test set and printing accuracy, recall and precision as well as the confusion matrix.\n",
    "#rf = RandomForestClassifier(criterion = \"gini\", n_estimators = 150, max_depth = 15, max_features = 0.5, min_samples_leaf = 1, min_samples_split = 4)\n",
    "#rf.fit(x_train, y_train)\n",
    "predictions = clf.best_estimator_.predict(x_test)\n",
    "acc_rf = round(clf.best_estimator_.score(x_test, y_test) * 100, 2)\n",
    "prec_rf = precision_score(y_test, predictions) * 100\n",
    "recall_rf = recall_score(y_test, predictions) * 100\n",
    "\n",
    "print(\"accuracy:\", acc_rf)\n",
    "print(\"Precision:\", prec_rf)\n",
    "print(\"Recall:\", recall_rf)\n",
    "confusion_matrix(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preperation for finding the learning curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(rf, x_train, y_train, cv = 5, scoring = \"accuracy\", train_sizes = np.linspace(0.01,1,30), verbose = 1)\n",
    "\n",
    "train_mean = np.mean(train_scores, axis = 1)\n",
    "test_mean = np.mean(test_scores, axis = 1)\n",
    "\n",
    "#Plotting the learning curve\n",
    "plt.plot(train_sizes, train_mean, label = 'training score')\n",
    "plt.plot(train_sizes, test_mean, label = 'cross-validation score')\n",
    "plt.grid()\n",
    "\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"samples\")\n",
    "plt.ylabel(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importances of the different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the importance of each features for the random forest model\n",
    "importances = pd.DataFrame({'feature':x_train.columns,'importance':np.round(rf.feature_importances_,4)})\n",
    "importances = importances.sort_values('importance',ascending=False).set_index('feature')\n",
    "importances.head(15)\n",
    "importances.plot.bar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
