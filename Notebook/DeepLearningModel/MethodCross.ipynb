{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    df_test = pd.read_csv(r'../test_data_clean', sep = \",\", engine='python')\n",
    "    df_train = pd.read_csv(r'../train_data_clean', sep = \",\", engine='python')\n",
    "    \n",
    "    #Removing unwanted column\n",
    "    test = df_test.drop(df_test.columns[0],axis=1)\n",
    "    train = df_train.drop(df_train.columns[0],axis=1)\n",
    "    \n",
    "    #Seperating \"target\" into own dataframe\n",
    "    test_targets = test.iloc[:,[13]]\n",
    "    test_inputs = test.loc[:,test.columns != 'target']\n",
    "    train_targets = train.iloc[:,[13]]\n",
    "    train_inputs = train.loc[:,train.columns != 'target']\n",
    "    \n",
    "    #Saving the data, not necessary to do this every time, but it is implemented in the method to show the entire process\n",
    "    np.savez('Spotify_data_train', inputs=train_inputs, targets=train_targets)\n",
    "    np.savez('Spotify_data_test', inputs=test_inputs, targets=test_targets)\n",
    "\n",
    "    npz = np.load('Spotify_data_train.npz')\n",
    "    train_inputs,train_targets = npz['inputs'].astype(np.float),npz['targets'].astype(np.int)\n",
    "\n",
    "    npz = np.load('Spotify_data_test.npz')\n",
    "    test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "\n",
    "    #Preprocessing using the sklearn method\n",
    "    test_inputs = preprocessing.scale(test_inputs)\n",
    "    \n",
    "    return train_inputs,train_targets,test_inputs,test_targets\n",
    "\n",
    "train_inputs,train_targets,test_inputs,test_targets = process_data()\n",
    "total_samples = len(train_inputs)\n",
    "split = math.ceil(total_samples/5)\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method for implementing crossvalidation, takes as input the train inputs and train targets. \n",
    "#Returns the training set and validation set depending on which fold to use. \n",
    "\n",
    "def crossFold(train_inputs,train_targets,i):\n",
    "    \n",
    "    if(i==0):\n",
    "        #Doesnt preprocess the data until it is seperated into validation and training set, to prevent data leakage. \n",
    "        validation_inputs = preprocessing.scale(train_inputs[:split])\n",
    "        validation_targets = train_targets[:split]\n",
    "        inputs = preprocessing.scale(train_inputs[split:])\n",
    "        targets = train_targets[split:]\n",
    "    elif(i == 4):\n",
    "        validation_inputs = preprocessing.scale(train_inputs[i*split:])\n",
    "        validation_targets = train_targets[i*split:]\n",
    "        inputs = preprocessing.scale(train_inputs[:i*split])\n",
    "        targets = train_targets[:i*split]\n",
    "    else:\n",
    "        i1 = train_inputs[:i*split]\n",
    "        i2 = train_inputs[(i+1)*split:]\n",
    "        inputs = preprocessing.scale(np.concatenate((i1,i2)))\n",
    "        \n",
    "        it1 = train_targets[:i*split]\n",
    "        it2 = train_targets[(i+1)*split:]\n",
    "        targets = np.concatenate((it1,it2))\n",
    "        \n",
    "        validation_inputs = preprocessing.scale(train_inputs[i*split:(i+1)*split])\n",
    "        validation_targets = train_targets[i*split:(i+1)*split]\n",
    "        \n",
    "    return inputs,targets,validation_inputs,validation_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Setting the parameter values we are trying to test\n",
    "\n",
    "number_of_layers = [1,2,5]\n",
    "number_of_nodes = [10,17,30]\n",
    "optimizers = ['adam','RMSprop','sgd']\n",
    "batch_sizes = [10,50,100]\n",
    "max_epochs = [5,10,25]\n",
    "regulizer_param = [0.001,0.01]\n",
    "\n",
    "\n",
    "\n",
    "HP_NUM_LAYERS = hp.HParam('num_layers', hp.Discrete(number_of_layers))\n",
    "HP_NUM_NODES = hp.HParam('num_nodes', hp.Discrete(number_of_nodes)) \n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(optimizers))\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete(batch_sizes)) \n",
    "HP_MAX_EPOCHS = hp.HParam('max_epochs', hp.Discrete(max_epochs))\n",
    "HP_L2 = hp.HParam('l2 regularizer', hp.RealInterval(regulizer_param[0],regulizer_param[1]))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "#Creates a summary file writer for the given log directory.\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_NUM_LAYERS,HP_NUM_NODES,HP_OPTIMIZER,HP_BATCH_SIZE, HP_MAX_EPOCHS,HP_L2], \n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "  )\n",
    "   \n",
    "        \n",
    "def train_test_model(run_dir,hparams,t_inputs,t_targets,v_inputs,v_targets): \n",
    "    \n",
    "    \n",
    "    input_size = 17 \n",
    "    output_size = 1\n",
    "    \n",
    "    #Builds and compiles the model with the specified metrics. \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(hparams[HP_NUM_NODES], activation='relu',input_shape=(input_size,)))\n",
    "    for i in range(hparams[HP_NUM_LAYERS]-1):\n",
    "        model.add(layers.Dense(hparams[HP_NUM_NODES],kernel_regularizer=tf.keras.regularizers.l2(hparams[HP_L2]), activation='relu'))\n",
    "    model.add(layers.Dense(output_size, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=hparams[HP_OPTIMIZER], loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
    "    \n",
    "       \n",
    "    batch_size = hparams[HP_BATCH_SIZE]\n",
    "    max_epochs = hparams[HP_MAX_EPOCHS]\n",
    "    \n",
    "    #Callbacks for logging of hyperparameters, metrics and early stopping. \n",
    "    callbacksList = [\n",
    "        tf.keras.callbacks.TensorBoard(run_dir,histogram_freq=1,update_freq=1,embeddings_freq=1),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=8, mode='max',baseline=None),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=8, verbose=0, mode='min',baseline=0.6)\n",
    "    ]\n",
    "    #Training the model. \n",
    "    history = model.fit(  t_inputs,t_targets,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=max_epochs, \n",
    "                          validation_data=(v_inputs, v_targets),\n",
    "                          callbacks=callbacksList,\n",
    "                          verbose = 0) \n",
    "\n",
    "    test_loss,test_accuracy = model.evaluate(test_inputs, test_targets)\n",
    "    \n",
    "    \n",
    "    return test_loss, test_accuracy   \n",
    "    \n",
    "#Writes accuracy and loss return by model.evaluate, for each run. Is logged in Tensorboard\n",
    "def run(run_dir,hparams,t_inputs,t_targets,v_inputs,v_targets):\n",
    "    with tf.summary.create_file_writer(run_dir+\"/Kfold\"+str(i)).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        accuracy = train_test_model(run_dir+\"/Kfold\"+str(i),hparams,t_inputs,t_targets,v_inputs,v_targets)\n",
    "       \n",
    "        tf.summary.scalar(\"loss\",accuracy[0],step=session_num)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy[1], step=session_num)\n",
    "   \n",
    "    #Return accuracy to be able to calculate the combined accuracy for the given hyperparameters.\n",
    "    return accuracy[1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "\n",
    "for num_layers in HP_NUM_LAYERS.domain.values:\n",
    "    for num_nodes in HP_NUM_NODES.domain.values:\n",
    "        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "            for batch_size in HP_BATCH_SIZE.domain.values:\n",
    "                for max_epoch in HP_MAX_EPOCHS.domain.values:\n",
    "                        for l2 in (HP_L2.domain.min_value, HP_L2.domain.max_value):\n",
    "                            \n",
    "                            #Defines dictionary of hyperparameters.\n",
    "                            hparams = {\n",
    "                                HP_NUM_LAYERS: num_layers,\n",
    "                                HP_NUM_NODES: num_nodes,\n",
    "                                HP_OPTIMIZER: optimizer,\n",
    "                                HP_BATCH_SIZE: batch_size, \n",
    "                                HP_MAX_EPOCHS: max_epoch,\n",
    "                                HP_L2: l2,\n",
    "                            }\n",
    "                            combined_acc = 0\n",
    "                            \n",
    "                            start_time_run = datetime.datetime.now()\n",
    "                            \n",
    "                            for i in range(5):\n",
    "                                start_time_fold = datetime.datetime.now()\n",
    "                                t_inputs,t_targets,v_inputs,v_targets = crossFold(train_inputs,train_targets,i)\n",
    "                                run_name = \"run-%d\" % session_num\n",
    "                                print('----------- Starting trial: %s' % run_name + '-----------')\n",
    "                                print({h.name: hparams[h] for h in hparams})\n",
    "                                print(\"Fold nr \"+str(i+1))\n",
    "                                \n",
    "                                accuracy = run('logs/hparam_tuning/' + run_name, hparams,t_inputs,t_targets,v_inputs,v_targets)\n",
    "                                combined_acc += accuracy\n",
    "                                \n",
    "                        \n",
    "                                finished_time = datetime.datetime.now()\n",
    "                                \n",
    "                                print('Total time elapsed: ', finished_time-start,'\\n')\n",
    "                                print('Time elapsed for fold: ', finished_time-start_time_fold ,'\\n')\n",
    "                            \n",
    "                            combined_acc = combined_acc/k\n",
    "                            tf.summary.scalar('Combined accuray:', combined_acc , step=1)\n",
    "                            print(\"Combined accuray: \",combined_acc,'\\n')\n",
    "                            \n",
    "                            print('Time elapsed for run: ',finished_time-start_time_run)\n",
    "                            \n",
    "                            \n",
    "                            session_num += 1\n",
    "\n",
    "print(\"----- Session finished -----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
