{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import datetime\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    df_test = pd.read_csv(r'test_data.csv', sep = \",\", engine='python')\n",
    "    df_train = pd.read_csv(r'train_data.csv', sep = \",\", engine='python')\n",
    "\n",
    "    test = df_test.drop(df_test.columns[0],axis=1)\n",
    "\n",
    "    test_targets = test.iloc[:,[13]]\n",
    "    test_inputs = test.loc[:,test.columns != 'target']\n",
    "\n",
    "    train = df_train.drop(df_train.columns[0],axis=1)\n",
    "    train_targets = train.iloc[:,[13]]\n",
    "    train_inputs = train.loc[:,train.columns != 'target']\n",
    "    \n",
    "    np.savez('Spotify_data_train', inputs=train_inputs, targets=train_targets)\n",
    "    np.savez('Spotify_data_test', inputs=test_inputs, targets=test_targets)\n",
    "\n",
    "    npz = np.load('Spotify_data_train.npz')\n",
    "    train_inputs,train_targets = npz['inputs'].astype(np.float),npz['targets'].astype(np.int)\n",
    "\n",
    "    npz = np.load('Spotify_data_test.npz')\n",
    "    test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "\n",
    "    test_inputs = preprocessing.scale(test_inputs)\n",
    "    \n",
    "    return train_inputs,train_targets,test_inputs,test_targets\n",
    "\n",
    "train_inputs,train_targets,test_inputs,test_targets = process_data()\n",
    "total_samples = len(train_inputs)\n",
    "split = math.ceil(total_samples/5)\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossFold(train_inputs,train_targets,i):\n",
    "    \n",
    "    if(i==0):\n",
    "        validation_inputs = preprocessing.scale(train_inputs[:split])\n",
    "        validation_targets = train_targets[:split]\n",
    "        inputs = preprocessing.scale(train_inputs[split:])\n",
    "        targets = train_targets[split:]\n",
    "    elif(i == 4):\n",
    "        validation_inputs = preprocessing.scale(train_inputs[i*split:])\n",
    "        validation_targets = train_targets[i*split:]\n",
    "        inputs = preprocessing.scale(train_inputs[:i*split])\n",
    "        targets = train_targets[:i*split]\n",
    "    else:\n",
    "        i1 = train_inputs[:i*split]\n",
    "        i2 = train_inputs[(i+1)*split:]\n",
    "        inputs = preprocessing.scale(np.concatenate((i1,i2)))\n",
    "        \n",
    "        it1 = train_targets[:i*split]\n",
    "        it2 = train_targets[(i+1)*split:]\n",
    "        targets = np.concatenate((it1,it2))\n",
    "        \n",
    "        validation_inputs = preprocessing.scale(train_inputs[i*split:(i+1)*split])\n",
    "        validation_targets = train_targets[i*split:(i+1)*split]\n",
    "        \n",
    "    return inputs,targets,validation_inputs,validation_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Setting the parameter values we are trying to test\n",
    "\n",
    "\n",
    "HP_NUM_LAYERS = hp.HParam('num_layers', hp.Discrete([1,2,5])) \n",
    "HP_NUM_NODES = hp.HParam('num_nodes', hp.Discrete([10,19,30]))\n",
    " \n",
    "    \n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-2,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9)\n",
    "optimizers = [\n",
    "    #tf.keras.optimizers.SGD(learning_rate=lr_schedule),\n",
    "    #tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    'adam','sgd','RMSprop'\n",
    "]\n",
    "\n",
    "#HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam','sgd','RMSprop']))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(optimizers))\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([10,50,100])) \n",
    "HP_MAX_EPOCHS = hp.HParam('max_epochs', hp.Discrete([5,10,25])) \n",
    "\n",
    "HP_L2 = hp.HParam('l2 regularizer', hp.RealInterval(0.001,0.01))\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "###################\n",
    "METRICS = [\n",
    "    hp.Metric(\n",
    "        \"val_accuracy\", group=\"validation\", display_name=\"accuracy (val.)\",\n",
    "    ),\n",
    "    hp.Metric(\"val_loss\", group=\"validation\", display_name=\"loss (val.)\",),\n",
    "    hp.Metric(\n",
    "        \"accuracy\", group=\"train\", display_name=\"accuracy (train)\",\n",
    "    ),\n",
    "    hp.Metric(\"loss\", group=\"train\", display_name=\"loss (train)\",),\n",
    "]\n",
    "METRICS = np.asarray(METRICS)\n",
    "#######################\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_NUM_LAYERS,HP_NUM_NODES,HP_OPTIMIZER,HP_BATCH_SIZE, HP_MAX_EPOCHS,HP_L2], \n",
    "    #metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "    #metrics=METRICS,\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "  )\n",
    "   \n",
    "        \n",
    "def train_test_model(run_dir,hparams,t_inputs,t_targets,v_inputs,v_targets): \n",
    "    \n",
    "    \n",
    "    input_size = 18 \n",
    "    output_size = 1\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(hparams[HP_NUM_NODES], activation='relu',input_shape=(18,)))\n",
    "    for i in range(hparams[HP_NUM_LAYERS]-1):\n",
    "        #model.add(layers.Dense(hparams[HP_NUM_NODES],kernel_regularizer=tf.keras.regularizers.l2(0.001), activation='relu'))\n",
    "        model.add(layers.Dense(hparams[HP_NUM_NODES],kernel_regularizer=tf.keras.regularizers.l2(hparams[HP_L2]), activation='relu'))\n",
    "    model.add(layers.Dense(output_size, activation='sigmoid'))\n",
    "    \n",
    "    #model.compile(optimizer=hparams[HP_OPTIMIZER], loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(optimizer=hparams[HP_OPTIMIZER], loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
    "    \n",
    "    #logdir = run_dir + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    #tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=path, histogram_freq=1)\n",
    "\n",
    "    batch_size = hparams[HP_BATCH_SIZE]\n",
    "    max_epochs = hparams[HP_MAX_EPOCHS]\n",
    "    \n",
    "    callbacksList = [\n",
    "        tf.keras.callbacks.TensorBoard(run_dir,histogram_freq=1,update_freq=1,embeddings_freq=1),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=5, mode='max',baseline=None),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=7, verbose=0, mode='min',baseline=0.6)\n",
    "    ]\n",
    "        \n",
    "    history = model.fit(  t_inputs,t_targets,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=max_epochs, \n",
    "                          validation_data=(v_inputs, v_targets),\n",
    "                          callbacks=callbacksList\n",
    "                                #tf.keras.callbacks.TensorBoard(run_dir,histogram_freq=1,update_freq='epoch'),  # log metrics\n",
    "                                \n",
    "                                \n",
    "                                #hp.KerasCallback(run_dir, hparams),  # log hparams\n",
    "                                #tf.keras.callbacks.TensorBoard.on_epoch_end(epoch,logs=None)\n",
    "                                ,\n",
    "                              verbose = 0) \n",
    "\n",
    "    test_loss,test_accuracy = model.evaluate(test_inputs, test_targets)\n",
    "    \n",
    "    #test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)\n",
    "    \n",
    "    #train_loss = history.history['loss']\n",
    "    #print(train_loss,'train\\n')\n",
    "    #val_loss = history.history['val_loss']\n",
    "    #print(val_loss,'val \\n')\n",
    "    #tf.summary.scalar('runs_split{}'.format(session_num)+'Loss/train', train_loss)\n",
    "    #tf.summary.scalar('runs_split{}'.format(session_num)+'Loss/val', val_loss)\n",
    "    \n",
    "    \n",
    "    return test_loss, test_accuracy   \n",
    "    \n",
    "def run(run_dir,hparams,t_inputs,t_targets,v_inputs,v_targets):\n",
    "    with tf.summary.create_file_writer(run_dir+\"/Kfold\"+str(i)).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        accuracy = train_test_model(run_dir+\"/Kfold\"+str(i),hparams,t_inputs,t_targets,v_inputs,v_targets)\n",
    "        #\n",
    "        #accuracy= tf.reshape(tf.convert_to_tensor(accuracy), []).numpy()\n",
    "        #\n",
    "        tf.summary.scalar(\"loss\",accuracy[0],step=session_num)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy[1], step=session_num)\n",
    "\n",
    "    return accuracy[1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n"
     ]
    }
   ],
   "source": [
    "print(len(HP_NUM_LAYERS.domain.values)*len(HP_NUM_NODES.domain.values)*len(HP_OPTIMIZER.domain.values)*len(HP_BATCH_SIZE.domain.values)*len(HP_MAX_EPOCHS.domain.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "\n",
    "for num_layers in HP_NUM_LAYERS.domain.values:\n",
    "    for num_nodes in HP_NUM_NODES.domain.values:\n",
    "        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "            for batch_size in HP_BATCH_SIZE.domain.values:\n",
    "                for max_epoch in HP_MAX_EPOCHS.domain.values:\n",
    "                        for l2 in (HP_L2.domain.min_value, HP_L2.domain.max_value):\n",
    "                            hparams = {\n",
    "                                HP_NUM_LAYERS: num_layers,\n",
    "                                HP_NUM_NODES: num_nodes,\n",
    "                                HP_OPTIMIZER: optimizer,\n",
    "                                HP_BATCH_SIZE: batch_size, \n",
    "                                HP_MAX_EPOCHS: max_epoch,\n",
    "                                HP_L2: l2,\n",
    "                            }\n",
    "                            combined_acc = 0\n",
    "                            \n",
    "                            start_time_run = datetime.datetime.now()\n",
    "                            \n",
    "                            for i in range(5):\n",
    "                                start_time_fold = datetime.datetime.now()\n",
    "                                t_inputs,t_targets,v_inputs,v_targets = crossFold(train_inputs,train_targets,i)\n",
    "                                run_name = \"run-%d\" % session_num\n",
    "                                print('----------- Starting trial: %s' % run_name + '-----------')\n",
    "                                print({h.name: hparams[h] for h in hparams})\n",
    "                                print(\"Fold nr \"+str(i+1))\n",
    "                                \n",
    "                                accuracy = run('logs/hparam_tuning/' + run_name, hparams,t_inputs,t_targets,v_inputs,v_targets)\n",
    "                                combined_acc += accuracy\n",
    "                                \n",
    "                                \n",
    "                                finished_time = datetime.datetime.now()\n",
    "                                \n",
    "                                print('Total time elapsed: ', finished_time-start,'\\n')\n",
    "                                print('Time elapsed for fold: ', finished_time-start_time_fold ,'\\n')\n",
    "                            #accuracy = train_test_model(hparams)\n",
    "                            #tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "                            combined_acc = combined_acc/k\n",
    "                            tf.summary.scalar('Combined accuray:', combined_acc , step=1)\n",
    "                            print(\"Combined accuray: \",combined_acc,'\\n')\n",
    "                            \n",
    "                            print('Time elapsed for run: ',finished_time-start_time_run)\n",
    "                            \n",
    "                            \n",
    "                            session_num += 1\n",
    "\n",
    "print(\"----- Session finished -----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
