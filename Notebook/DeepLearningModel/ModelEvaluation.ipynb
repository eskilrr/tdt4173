{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the methods chosen after hyperparametertuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Press Kernal -> Restart & Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix,recall_score,precision_score,f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    \n",
    "    df_test = pd.read_csv(r'../test_data_clean', sep = \",\", engine='python')\n",
    "    df_train = pd.read_csv(r'../train_data_clean', sep = \",\", engine='python')\n",
    "\n",
    "    #Removing unwanted column\n",
    "    test = df_test.drop(df_test.columns[0],axis=1)\n",
    "    train = df_train.drop(df_train.columns[0],axis=1)\n",
    "    \n",
    "    #Seperating \"target\" into own dataframe\n",
    "    test_targets = test.iloc[:,[13]]\n",
    "    test_inputs = test.loc[:,test.columns != 'target']    \n",
    "    train_targets = train.iloc[:,[13]]\n",
    "    train_inputs = train.loc[:,train.columns != 'target']\n",
    "     \n",
    "    #Saving the data, not necessary to do this every time, but it is implemented in the method to show the entire process    \n",
    "    np.savez('Spotify_data_train', inputs=train_inputs, targets=train_targets)\n",
    "    np.savez('Spotify_data_test', inputs=test_inputs, targets=test_targets)\n",
    "\n",
    "    npz = np.load('Spotify_data_train.npz')\n",
    "    train_inputs,train_targets = npz['inputs'].astype(np.float),npz['targets'].astype(np.int)\n",
    "\n",
    "    npz = np.load('Spotify_data_test.npz')\n",
    "    test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "    \n",
    "    #Preprocessing using the sklearn method\n",
    "    test_inputs = preprocessing.scale(test_inputs)\n",
    "    #train_inputs = preprocessing.scale(train_inputs)\n",
    "\n",
    "    return train_inputs,train_targets,test_inputs,test_targets\n",
    "\n",
    "train_inputs,train_targets,test_inputs,test_targets = process_data()\n",
    "\n",
    "total_samples = len(train_inputs)\n",
    "split = math.ceil(total_samples/5)\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_nodes,n_layers,regulizer_const):\n",
    "    \n",
    "    input_size = 17\n",
    "    output_size = 1\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    #Specifying input shape in the dense layer, Keras automaticly adds an input layer.\n",
    "    model.add(layers.Dense(n_nodes, activation='relu',input_shape=(input_size,)))\n",
    "    for i in range(n_layers-1):\n",
    "        model.add(layers.Dense(n_nodes,kernel_regularizer=tf.keras.regularizers.l2(regulizer_const), activation='relu'))\n",
    "    model.add(layers.Dense(output_size, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Method for implementing crossvalidation, takes as input the train inputs and train targets. \n",
    "#Returns the training set and validation set depending on which fold to use.\n",
    "def crossFold(train_inputs,train_targets,i):\n",
    "    \n",
    "    if(i==0):\n",
    "        #Doesnt preprocess the data until it is seperated into validation and training set, to prevent data leakage. \n",
    "        validation_inputs = preprocessing.scale(train_inputs[:split])\n",
    "        validation_targets = train_targets[:split]\n",
    "        inputs = preprocessing.scale(train_inputs[split:])\n",
    "        targets = train_targets[split:]\n",
    "    elif(i == 4):\n",
    "        validation_inputs = preprocessing.scale(train_inputs[i*split:])\n",
    "        validation_targets = train_targets[i*split:]\n",
    "        inputs = preprocessing.scale(train_inputs[:i*split])\n",
    "        targets = train_targets[:i*split]\n",
    "    else:\n",
    "        i1 = train_inputs[:i*split]\n",
    "        i2 = train_inputs[(i+1)*split:]\n",
    "        inputs = preprocessing.scale(np.concatenate((i1,i2)))\n",
    "        \n",
    "        it1 = train_targets[:i*split]\n",
    "        it2 = train_targets[(i+1)*split:]\n",
    "        targets = np.concatenate((it1,it2))\n",
    "        \n",
    "        validation_inputs = preprocessing.scale(train_inputs[i*split:(i+1)*split])\n",
    "        validation_targets = train_targets[i*split:(i+1)*split]\n",
    "        \n",
    "    return inputs,targets,validation_inputs,validation_targets\n",
    "    \n",
    "def compile_model(model,chosen_optimizer): \n",
    "    model.compile(optimizer=chosen_optimizer, loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
    "    \n",
    "def fit_model(model,b_size,m_epochs,t_inputs,t_targets,v_inputs,v_targets):\n",
    "    \n",
    "    \n",
    "    batch_size = b_size\n",
    "    max_epochs = m_epochs\n",
    "    \n",
    "    history = model.fit(  t_inputs,t_targets,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=max_epochs, \n",
    "                          validation_data=(v_inputs, v_targets),\n",
    "                          verbose = 0) \n",
    "    return history\n",
    "\n",
    "def evaluate_model(model):\n",
    "    test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)\n",
    "    print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))\n",
    "    return test_loss,test_accuracy\n",
    "    \n",
    "    \n",
    "def predict(model):\n",
    "    predictions = model.predict(test_inputs)\n",
    "    finished_predictions = []\n",
    "    for i in range(len(predictions)):\n",
    "        if(predictions[i]>0.5):\n",
    "            finished_predictions.append([1])\n",
    "        else:\n",
    "            finished_predictions.append([0])\n",
    "    return finished_predictions\n",
    "\n",
    "def build_single_model(n_nodes,n_layers,regulizer_const,optimizer,b_size,m_epochs):\n",
    "    model = build_model(n_nodes,n_layers,regulizer_const)\n",
    "    compile_model(model,optimizer)\n",
    "\n",
    "    for i in range(5):\n",
    "        print('Fold nr:',str(i+1))\n",
    "        t_inputs,t_targets,v_inputs,v_targets = crossFold(train_inputs,train_targets,i)\n",
    "        history = fit_model(model,b_size,m_epochs,t_inputs,t_targets,v_inputs,v_targets)\n",
    "    print('----Finished----')\n",
    "    evaluate_model(model)\n",
    "\n",
    "    predictions = predict(model)\n",
    "    \n",
    "    return model, predictions   \n",
    "\n",
    "def print_results(predictions,modelnr):\n",
    "    print('Model nr: ',str(modelnr) )\n",
    "    print(' \\nConfusion Matrix: \\n',confusion_matrix(test_targets, predictions))\n",
    "    print('\\nF1 Score:',f1_score(test_targets,predictions))\n",
    "    print('\\nRecall Score:',recall_score(test_targets,predictions))\n",
    "    print('\\nPrecision Score:',precision_score(test_targets,predictions))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build_single_model(n_nodes,n_layers,regulizer_const,optimizer,b_size,m_epochs)\n",
    "model1,predictions_model1 = build_single_model(30,1,0.01,'adam',50,25)\n",
    "print_results(predictions_model1,1)\n",
    "\n",
    "model2,predictions_model2 = build_single_model(30,2,0.001,'adam',10,25)\n",
    "print_results(predictions_model2,2)\n",
    "\n",
    "model3,predictions_model3 = build_single_model(30,1,0.01,'RMSprop',5,50)\n",
    "print_results(predictions_model3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 1: ',model1.summary(),'\\n')\n",
    "print('----')\n",
    "print('Model 2: ',model2.summary(),'\\n')\n",
    "print('----')\n",
    "print('Model 3: ',model3.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
