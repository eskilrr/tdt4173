{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import datetime\n",
    "from kerastuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track</th>\n",
       "      <th>artist</th>\n",
       "      <th>uri</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>...</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>chorus_hit</th>\n",
       "      <th>sections</th>\n",
       "      <th>target</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>artist_followers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lucky Man</td>\n",
       "      <td>Montgomery Gentry</td>\n",
       "      <td>spotify:track:4GiXBCUF7H6YfNQsnBRIzl</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.471</td>\n",
       "      <td>4</td>\n",
       "      <td>-7.27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.532</td>\n",
       "      <td>133.061</td>\n",
       "      <td>196707</td>\n",
       "      <td>4</td>\n",
       "      <td>30.88059</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>973747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       track             artist                                   uri  \\\n",
       "0  Lucky Man  Montgomery Gentry  spotify:track:4GiXBCUF7H6YfNQsnBRIzl   \n",
       "\n",
       "   danceability  energy  key  loudness  mode  speechiness  acousticness  ...  \\\n",
       "0         0.578   0.471    4     -7.27     1       0.0289         0.368  ...   \n",
       "\n",
       "   liveness  valence    tempo  duration_ms  time_signature  chorus_hit  \\\n",
       "0     0.159    0.532  133.061       196707               4    30.88059   \n",
       "\n",
       "   sections  target  artist_popularity  artist_followers  \n",
       "0        13       1                 63            973747  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../Data/CSV-files/dataset-of-00sv2.csv')\n",
    "#df = pd.read_csv('../../Data/CSV-files/dataset-of-90sv2.csv')\n",
    "#df = pd.read_csv('../../Data/CSV-files/dataset-of-60sv2.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dataframe):\n",
    "    data = dataframe.sample(frac=1)\n",
    "    data.drop([\"track\",\"artist\",\"uri\"],axis=1,inplace=True)\n",
    "    \n",
    "    target = data.iloc[:,[15]]\n",
    "    \n",
    "    #Fjerner kolonnen target\n",
    "    unscaled_inputs = data.loc[:,data.columns != 'target']\n",
    "    \n",
    "    #Skaler dataen, samler verdier rundt 0\n",
    "    scaled_inputs = preprocessing.scale(unscaled_inputs)\n",
    "    #scaled_inputs er typen array \n",
    "    \n",
    "    samples_count = scaled_inputs.shape[0]\n",
    "    train_samples_count = int(0.8*samples_count)\n",
    "    validation_samples_count = int(0.1*samples_count)\n",
    "    test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "    \n",
    "    train_inputs = scaled_inputs[:train_samples_count]\n",
    "    train_targets = target[:train_samples_count]\n",
    "\n",
    "    validation_inputs = scaled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "    validation_targets = target[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "    test_inputs = scaled_inputs[train_samples_count+validation_samples_count:]\n",
    "    test_targets = target[train_samples_count+validation_samples_count:]\n",
    "    \n",
    "    np.savez('Spotify_data_train', inputs=train_inputs, targets=train_targets)\n",
    "    np.savez('Spotify_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "    np.savez('Spotify_data_test', inputs=test_inputs, targets=test_targets)\n",
    "    \n",
    "    npz = np.load('Spotify_data_train.npz')\n",
    "    train_inputs,train_targets = npz['inputs'].astype(np.float),npz['targets'].astype(np.int)\n",
    "\n",
    "    npz = np.load('Spotify_data_validation.npz')\n",
    "    validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "\n",
    "    npz = np.load('Spotify_data_test.npz')\n",
    "    test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "    \n",
    "    return [train_inputs,train_targets,validation_inputs, validation_targets, test_inputs, test_targets ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definerer variabler som brukes senere:\n",
    "data = process_data(df)\n",
    "train_inputs = data[0]\n",
    "train_targets = data[1]\n",
    "validation_inputs = data[2] \n",
    "validation_targets = data[3]\n",
    "test_inputs = data[4]\n",
    "test_targets = data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Set the input and output sizes\n",
    "    input_size = 17 # count of features\n",
    "    output_size = 2 # count of targets\n",
    "    # Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "    hidden_layer_size = 50 # counts of neurons\n",
    "\n",
    "    # define how the model will look like\n",
    "    #Stacker ulike lak med hidden layer \"oppå\" hverandre i modellen \n",
    "    model = tf.keras.Sequential([\n",
    "        #tf.keras.Input(shape=(17,)),\n",
    "        #tf.keras.layers.Dense(input_size=)?????\n",
    "        # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "        # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu',input_shape=(17,)), # 1st hidden layer\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3nd hidden layer\n",
    "        # the final layer is no different, we just make sure to activate it with softmax\n",
    "        tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "    ])\n",
    "    print(model.output_shape)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2)\n",
      "Epoch 1/10\n",
      "16/16 - 0s - loss: 0.6340 - accuracy: 0.6794 - val_loss: 0.5321 - val_accuracy: 0.8348\n",
      "Epoch 2/10\n",
      "16/16 - 0s - loss: 0.4502 - accuracy: 0.8382 - val_loss: 0.3649 - val_accuracy: 0.8807\n",
      "Epoch 3/10\n",
      "16/16 - 0s - loss: 0.3309 - accuracy: 0.8565 - val_loss: 0.2887 - val_accuracy: 0.8859\n",
      "Epoch 4/10\n",
      "16/16 - 0s - loss: 0.2960 - accuracy: 0.8716 - val_loss: 0.2763 - val_accuracy: 0.8978\n",
      "Epoch 5/10\n",
      "16/16 - 0s - loss: 0.2821 - accuracy: 0.8772 - val_loss: 0.2763 - val_accuracy: 0.8876\n",
      "Epoch 6/10\n",
      "16/16 - 0s - loss: 0.2711 - accuracy: 0.8833 - val_loss: 0.2626 - val_accuracy: 0.9029\n",
      "Epoch 7/10\n",
      "16/16 - 0s - loss: 0.2627 - accuracy: 0.8863 - val_loss: 0.2609 - val_accuracy: 0.9029\n",
      "Epoch 8/10\n",
      "16/16 - 0s - loss: 0.2571 - accuracy: 0.8904 - val_loss: 0.2553 - val_accuracy: 0.9012\n",
      "Epoch 9/10\n",
      "16/16 - 0s - loss: 0.2505 - accuracy: 0.8925 - val_loss: 0.2642 - val_accuracy: 0.8995\n",
      "Epoch 10/10\n",
      "16/16 - 0s - loss: 0.2487 - accuracy: 0.8938 - val_loss: 0.2545 - val_accuracy: 0.9029\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "compile_model(model)\n",
    "history = fit_model(model)\n",
    "#plot_history(history)\n",
    "#evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6339990496635437,\n",
       " 0.4502210319042206,\n",
       " 0.3309062421321869,\n",
       " 0.2960009276866913,\n",
       " 0.2821185290813446,\n",
       " 0.27109113335609436,\n",
       " 0.26269280910491943,\n",
       " 0.2570542097091675,\n",
       " 0.2504764497280121,\n",
       " 0.24868358671665192]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1723059   0.07915099 -0.13768633 ...  0.15436186  0.15577485\n",
      "  -0.08993502]\n",
      " [-0.07091202  0.15922083 -0.16688454 ... -0.15305474  0.1783848\n",
      "  -0.1739765 ]\n",
      " [ 0.08919661 -0.06042151  0.22906126 ...  0.13217781 -0.02420439\n",
      "  -0.02833499]\n",
      " ...\n",
      " [ 0.17559706 -0.1208756  -0.18614784 ... -0.06117219 -0.23449816\n",
      "  -0.13812485]\n",
      " [ 0.01603837  0.23396577  0.10057767 ... -0.14579213 -0.09695637\n",
      "   0.1274346 ]\n",
      " [ 0.16235347  0.09053053  0.02122171 ...  0.17206202  0.24406888\n",
      "  -0.22176045]]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#model.summary()\n",
    "weights = model.layers[1].get_weights()[0]\n",
    "biases = model.layers[3].get_weights()[1]\n",
    "print(weights)\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(4, activation='relu',input_shape=(19,)))\n",
    "    print(model.output_shape)\n",
    "    for i in range(2):\n",
    "        model.add(tf.keras.layers.Dense(4,kernel_regularizer=tf.keras.regularizers.l2(0.001), activation='relu'))\n",
    "        print(model.output_shape)\n",
    "    model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "    print(model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    #model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=path, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kan også endre learning rate sånn:\n",
    "#model.optimizer.lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model):\n",
    "    # That's where we train the model we have built.\n",
    "    # set the batch size\n",
    "    batch_size = 300\n",
    "    # set a maximum number of training epochs\n",
    "    max_epochs = 10\n",
    "\n",
    "    # fit the model\n",
    "    # note that this time the train, validation and test data are not iterable\n",
    "    history = model.fit(  train_inputs, # train inputs\n",
    "                          train_targets, # train targets\n",
    "                          batch_size=batch_size, # batch size\n",
    "                          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n",
    "                          # callbacks are functions called by a task when a task is completed\n",
    "                          # task here is to check if val_loss is increasing\n",
    "                          #callbacks=[early_stopping], # early stopping\n",
    "                          validation_data=(validation_inputs, validation_targets),# validation data\n",
    "                          #callbacks=[tensorboard_callback],\n",
    "                          verbose = 2# making sure we get enough information about the training process\n",
    "              ) \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # Get training and test loss histories\n",
    "    training_loss = history.history['loss']\n",
    "    validation_loss = history.history['val_loss']\n",
    "\n",
    "    accuracy = history.history['accuracy']\n",
    "\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "    # Visualize loss history\n",
    "    plt.plot(epoch_count, training_loss, 'r--')\n",
    "    plt.plot(epoch_count, validation_loss, 'b-')\n",
    "    plt.plot(epoch_count,accuracy)\n",
    "    plt.legend(['Training Loss', 'Validation Loss','Accuracy'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)\n",
    "    print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "compile_model(model)\n",
    "history = fit_model(model)\n",
    "plot_history(history)\n",
    "evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = model.evaluate(test_inputs, test_targets)\n",
    "#print(model.metrics_names)\n",
    "\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_accuracy)\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    true_label = test_targets[i]\n",
    "    plt.grid(False)\n",
    "\n",
    "    thisplot = plt.bar(range(2), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    \n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    \n",
    "    plt.ylabel(\"{:2.0f}%\".format(100*np.max(predictions_array),color=color))\n",
    "\n",
    "    \n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label[0]].set_color('blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(rows,cols):\n",
    "    plt.figure(figsize=(2*2*cols, 2*rows))\n",
    "    for i in range(rows*cols):\n",
    "        plt.subplot(rows, 2*cols, 2*i+2)\n",
    "        plot_value_array(i, predictions[i], test_targets)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([model,tf.keras.layers.Softmax()])\n",
    "predictions = probability_model.predict(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wrong_guess_data():\n",
    "    wrong_guess = []\n",
    "    for i in range(100):\n",
    "        index = np.argmax(predictions[i])\n",
    "        if(index!=test_targets[i]):\n",
    "            arr = np.concatenate((test_inputs[i],test_targets[i])) \n",
    "            wrong_guess.append(arr)\n",
    "            #wrong_guess.append(test_targets[i])\n",
    "    return wrong_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network(path):\n",
    "    df = pd.read_csv(path)\n",
    "    data = process_data(df)\n",
    "    train_inputs = data[0]\n",
    "    train_targets = data[1]\n",
    "    validation_inputs = data[2] \n",
    "    validation_targets = data[3]\n",
    "    test_inputs = data[4]\n",
    "    test_targets = data[5]\n",
    "    model = build_model()\n",
    "    compile_model(model)\n",
    "    history = fit_model(model)\n",
    "    plot_history(history)\n",
    "    evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_network('../../Data/CSV-files/dataset-of-00sv2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
