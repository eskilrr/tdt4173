{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Data/CSV-files/dataset-of-00sv2.csv')\n",
    "#df = pd.read_csv('../../Data/CSV-files/dataset-of-90sv2.csv')\n",
    "#df = pd.read_csv('../../Data/CSV-files/dataset-of-60sv2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.sample(frac=1)\n",
    "data.drop([\"track\",\"artist\",\"uri\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alle rader og bare den siste kollonnen\n",
    "target = data.iloc[:,[15]]\n",
    "#Henter ut alle rader, og alle kollonner fra kol: 0 til og med nest siste \n",
    "#columns_dont_want = [\"target\"]\n",
    "#select = [x for x in data.columns if x not in columns_dont_want]\n",
    "#unscaled_inputs = data.iloc[:,select]\n",
    "\n",
    "unscaled_inputs = data.loc[:, data.columns != \"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Samler verdiene rundt 0, \n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count = scaled_inputs.shape[0]\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = scaled_inputs[:train_samples_count]\n",
    "train_targets = target[:train_samples_count]\n",
    "\n",
    "validation_inputs = scaled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = target[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "test_inputs = scaled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = target[train_samples_count+validation_samples_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_inputs',train_inputs.shape)\n",
    "print('train_targets',train_targets.shape)\n",
    "print('validation_inputs',validation_inputs.shape )\n",
    "print('validation_targets',validation_targets.shape)\n",
    "print('test_inputs', test_inputs.shape)\n",
    "print('test_targets',test_targets.shape)\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hvorfor lagre det som fil og hente ut verdier slik det gjøre i denne og neste blokk?\n",
    "np.savez('Spotify_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Spotify_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Spotify_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Spotify_data_train.npz')\n",
    "#Henter ut fra filer med navnet vi lagret det med over\n",
    "train_inputs = npz['inputs'].astype(np.float)\n",
    "train_targets = npz['targets'].astype(np.int)\n",
    "\n",
    "npz = np.load('Spotify_data_validation.npz')\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "\n",
    "npz = np.load('Spotify_data_test.npz')\n",
    "test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input and output sizes\n",
    "input_size = 17 # count of features\n",
    "output_size = 2 # count of targets\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 50 # counts of neurons\n",
    "\n",
    "# define how the model will look like\n",
    "#Stacker ulike lak med hidden layer \"oppå\" hverandre i modellen \n",
    "model = tf.keras.Sequential([\n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3nd hidden layer\n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SJEKK HVA DE ULIKE HER GJØR, PRØV FORSKJELLIGE \n",
    "#Optimizer er gradient descent?\n",
    "\n",
    "# we define the optimizer we'd like to use, \n",
    "# the loss function, \n",
    "# and the metrics we are interested in obtaining at each iteration\n",
    "#custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's where we train the model we have built.\n",
    "# set the batch size\n",
    "batch_size = 300\n",
    "# set a maximum number of training epochs\n",
    "max_epochs = 10\n",
    "\n",
    "# fit the model\n",
    "# note that this time the train, validation and test data are not iterable\n",
    "history = model.fit(  train_inputs, # train inputs\n",
    "                      train_targets, # train targets\n",
    "                      batch_size=batch_size, # batch size\n",
    "                      epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n",
    "                      # callbacks are functions called by a task when a task is completed\n",
    "                      # task here is to check if val_loss is increasing\n",
    "                      #callbacks=[early_stopping], # early stopping\n",
    "                      validation_data=(validation_inputs, validation_targets), # validation data\n",
    "                      verbose = 2 # making sure we get enough information about the training process\n",
    "          )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "accuracy = history.history['accuracy']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, validation_loss, 'b-')\n",
    "plt.plot(epoch_count,accuracy)\n",
    "plt.legend(['Training Loss', 'Validation Loss','Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)\n",
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([model, \n",
    "                                         tf.keras.layers.Softmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = probability_model.predict(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0])\n",
    "print(test_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    true_label = test_targets[i]\n",
    "    plt.grid(False)\n",
    "\n",
    "    thisplot = plt.bar(range(2), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    \n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    \n",
    "    plt.ylabel(\"{:2.0f}%\".format(100*np.max(predictions_array),color=color))\n",
    "\n",
    "    \n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label[0]].set_color('blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 105\n",
    "plt.figure(figsize=(6,3))\n",
    "plot_value_array(i, predictions[i],  test_targets)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 10\n",
    "num_cols = 10\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "    plot_value_array(i, predictions[i], test_targets)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_guess = []\n",
    "for i in range(100):\n",
    "    index = np.argmax(predictions[i])\n",
    "    if(index!=test_targets[i]):\n",
    "        wrong_guess.append(test_inputs[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(predictions[2]))\n",
    "print(predictions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Burde egentlig ha med alle feltene her(mangle 'target','weeks'?)\n",
    "df = pd.DataFrame(wrong_guess)\n",
    "df = df.rename(columns={0:'danceability',1:'energy',2:'key',3:'loudness',4:'mode',5:'speechiness',\n",
    "       6:'acousticness',7:'instrumentalness',8:'liveness',9:'valence',10:'tempo',\n",
    "       11:'duration_ms',12:'time_signature',13:'chorus_hit',14:'sections',\n",
    "       15:'artist_popularity',16:'artist_followers'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lav acousticness, speechness, instrumentalness, liveness, få followers\n",
    "for i in df.columns:\n",
    "    plt.hist(df[i])\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    for j in df.columns:\n",
    "        sns.scatterplot(x=i ,y=j, data =df)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1,2],[3,5,6,7,8]]\n",
    "print(np.argmax(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
