{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track</th>\n",
       "      <th>artist</th>\n",
       "      <th>uri</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>...</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>chorus_hit</th>\n",
       "      <th>sections</th>\n",
       "      <th>target</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>artist_followers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lucky Man</td>\n",
       "      <td>Montgomery Gentry</td>\n",
       "      <td>spotify:track:4GiXBCUF7H6YfNQsnBRIzl</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.471</td>\n",
       "      <td>4</td>\n",
       "      <td>-7.27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.532</td>\n",
       "      <td>133.061</td>\n",
       "      <td>196707</td>\n",
       "      <td>4</td>\n",
       "      <td>30.88059</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>973747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       track             artist                                   uri  \\\n",
       "0  Lucky Man  Montgomery Gentry  spotify:track:4GiXBCUF7H6YfNQsnBRIzl   \n",
       "\n",
       "   danceability  energy  key  loudness  mode  speechiness  acousticness  ...  \\\n",
       "0         0.578   0.471    4     -7.27     1       0.0289         0.368  ...   \n",
       "\n",
       "   liveness  valence    tempo  duration_ms  time_signature  chorus_hit  \\\n",
       "0     0.159    0.532  133.061       196707               4    30.88059   \n",
       "\n",
       "   sections  target  artist_popularity  artist_followers  \n",
       "0        13       1                 63            973747  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../Data/CSV-files/dataset-of-00sv2.csv')\n",
    "#df = pd.read_csv('../../Data/CSV-files/dataset-of-90sv2.csv')\n",
    "#df = pd.read_csv('../../Data/CSV-files/dataset-of-60sv2.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(dataframe):\n",
    "    return dataframe.sample(frac=1)\n",
    "\n",
    "def remove_data(dataframe,list):\n",
    "    return dataframe.drop(list,axis=1,inplace=True)\n",
    "\n",
    "def remove_column(dataframe,columnName):\n",
    "    return dataframe.loc[:,dataframe.columns != columnName]\n",
    "\n",
    "def get_column(dataframe,columnNr):\n",
    "    return dataframe.iloc[:,[columnNr]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorterer dataen og tar bort kolonner\n",
    "data = sort_data(df)\n",
    "remove_data(data,[\"track\",\"artist\",\"uri\"])\n",
    "\n",
    "#Henter ut targetverdiene til egen dataframe\n",
    "target = get_column(data,15)\n",
    "\n",
    "#Fjerner kolonnen target\n",
    "unscaled_inputs = remove_column(data,'target')\n",
    "\n",
    "#Skaler dataen, samler verdier rundt 0\n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs)\n",
    "#scaled_inputs er typen array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Henter ut alle rader, og alle kollonner fra kol: 0 til og med nest siste \n",
    "#columns_dont_want = [\"target\"]\n",
    "#select = [x for x in data.columns if x not in columns_dont_want]\n",
    "#unscaled_inputs = data.iloc[:,select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count = scaled_inputs.shape[0]\n",
    "\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listeopersjoner på scaled_inputs og target, deler dataen inn i trening, validering og test - datasett\n",
    "\n",
    "train_inputs = scaled_inputs[:train_samples_count]\n",
    "train_targets = target[:train_samples_count]\n",
    "\n",
    "validation_inputs = scaled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = target[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "test_inputs = scaled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = target[train_samples_count+validation_samples_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training \\n\" ,np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count,\"\\n\")\n",
    "print(\"Validation \\n\",np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count,\"\\n\")\n",
    "print(\"Test \\n\",np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hvorfor lagre det som fil og hente ut verdier slik det gjøre i denne og neste blokk?\n",
    "np.savez('Spotify_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Spotify_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Spotify_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Spotify_data_train.npz')\n",
    "#Henter ut fra filer med navnet vi lagret det med over\n",
    "train_inputs = npz['inputs'].astype(np.float)\n",
    "train_targets = npz['targets'].astype(np.int)\n",
    "\n",
    "npz = np.load('Spotify_data_validation.npz')\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "\n",
    "npz = np.load('Spotify_data_test.npz')\n",
    "test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input and output sizes\n",
    "input_size = 17 # count of features\n",
    "output_size = 2 # count of targets\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 50 # counts of neurons\n",
    "\n",
    "# define how the model will look like\n",
    "#Stacker ulike lak med hidden layer \"oppå\" hverandre i modellen \n",
    "model = tf.keras.Sequential([\n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3nd hidden layer\n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SJEKK HVA DE ULIKE HER GJØR, PRØV FORSKJELLIGE \n",
    "#Optimizer er gradient descent?\n",
    "\n",
    "# we define the optimizer we'd like to use, \n",
    "# the loss function, \n",
    "# and the metrics we are interested in obtaining at each iteration\n",
    "#custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0005s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "16/16 - 0s - loss: 0.5818 - accuracy: 0.7136 - val_loss: 0.4788 - val_accuracy: 0.7853\n",
      "Epoch 2/10\n",
      "16/16 - 0s - loss: 0.4178 - accuracy: 0.8254 - val_loss: 0.3759 - val_accuracy: 0.8330\n",
      "Epoch 3/10\n",
      "16/16 - 0s - loss: 0.3291 - accuracy: 0.8610 - val_loss: 0.3440 - val_accuracy: 0.8535\n",
      "Epoch 4/10\n",
      "16/16 - 0s - loss: 0.2921 - accuracy: 0.8797 - val_loss: 0.3345 - val_accuracy: 0.8688\n",
      "Epoch 5/10\n",
      "16/16 - 0s - loss: 0.2789 - accuracy: 0.8833 - val_loss: 0.3284 - val_accuracy: 0.8722\n",
      "Epoch 6/10\n",
      "16/16 - 0s - loss: 0.2688 - accuracy: 0.8874 - val_loss: 0.3239 - val_accuracy: 0.8688\n",
      "Epoch 7/10\n",
      "16/16 - 0s - loss: 0.2612 - accuracy: 0.8880 - val_loss: 0.3212 - val_accuracy: 0.8739\n",
      "Epoch 8/10\n",
      "16/16 - 0s - loss: 0.2540 - accuracy: 0.8961 - val_loss: 0.3188 - val_accuracy: 0.8654\n",
      "Epoch 9/10\n",
      "16/16 - 0s - loss: 0.2480 - accuracy: 0.8963 - val_loss: 0.3173 - val_accuracy: 0.8654\n",
      "Epoch 10/10\n",
      "16/16 - 0s - loss: 0.2425 - accuracy: 0.8997 - val_loss: 0.3132 - val_accuracy: 0.8722\n"
     ]
    }
   ],
   "source": [
    "# That's where we train the model we have built.\n",
    "# set the batch size\n",
    "batch_size = 300\n",
    "# set a maximum number of training epochs\n",
    "max_epochs = 10\n",
    "\n",
    "# fit the model\n",
    "# note that this time the train, validation and test data are not iterable\n",
    "history = model.fit(  train_inputs, # train inputs\n",
    "                      train_targets, # train targets\n",
    "                      batch_size=batch_size, # batch size\n",
    "                      epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n",
    "                      # callbacks are functions called by a task when a task is completed\n",
    "                      # task here is to check if val_loss is increasing\n",
    "                      #callbacks=[early_stopping], # early stopping\n",
    "                      validation_data=(validation_inputs, validation_targets), # validation data\n",
    "                      verbose = 2 # making sure we get enough information about the training process\n",
    "          )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5817580223083496,\n",
       " 0.417798787355423,\n",
       " 0.3290999233722687,\n",
       " 0.29205775260925293,\n",
       " 0.2788926959037781,\n",
       " 0.2688109874725342,\n",
       " 0.2611764073371887,\n",
       " 0.2539537250995636,\n",
       " 0.24802672863006592,\n",
       " 0.24247586727142334]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "accuracy = history.history['accuracy']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, validation_loss, 'b-')\n",
    "plt.plot(epoch_count,accuracy)\n",
    "plt.legend(['Training Loss', 'Validation Loss','Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)\n",
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([model, \n",
    "                                         tf.keras.layers.Softmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = probability_model.predict(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0])\n",
    "print(test_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs[0]\n",
    "test_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    true_label = test_targets[i]\n",
    "    plt.grid(False)\n",
    "\n",
    "    thisplot = plt.bar(range(2), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    \n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    \n",
    "    plt.ylabel(\"{:2.0f}%\".format(100*np.max(predictions_array),color=color))\n",
    "\n",
    "    \n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label[0]].set_color('blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 105\n",
    "plt.figure(figsize=(6,3))\n",
    "plot_value_array(i, predictions[i],  test_targets)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 10\n",
    "num_cols = 10\n",
    "num_images = num_rows*num_cols\n",
    "\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "    plot_value_array(i, predictions[i], test_targets)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_guess = []\n",
    "for i in range(100):\n",
    "    index = np.argmax(predictions[i])\n",
    "    if(index!=test_targets[i]):\n",
    "        wrong_guess.append(test_inputs[i])\n",
    "        wrong.guess.append(test_targets[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(predictions[2]))\n",
    "print(predictions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Burde egentlig ha med alle feltene her(mangle 'target','weeks'?)\n",
    "df = pd.DataFrame(wrong_guess)\n",
    "df = df.rename(columns={0:'danceability',1:'energy',2:'key',3:'loudness',4:'mode',5:'speechiness',\n",
    "       6:'acousticness',7:'instrumentalness',8:'liveness',9:'valence',10:'tempo',\n",
    "       11:'duration_ms',12:'time_signature',13:'chorus_hit',14:'sections',\n",
    "       15:'artist_popularity',16:'artist_followers'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lav acousticness, speechness, instrumentalness, liveness, få followers\n",
    "for i in df.columns:\n",
    "    plt.hist(df[i])\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    for j in df.columns:\n",
    "        sns.scatterplot(x=i ,y=j, data =df)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
